[
    {
        "author": "LinOnetwo",
        "dependents": "$:/plugins/linonetwo/markdown-transformer",
        "description": "Chat with TidGi's build-in language model service (LLama/Rwkv) in Tiddlywiki. A private, local and rooted ChatGPT AI.",
        "list": "readme config",
        "name": "ChatGPT",
        "plugin-type": "plugin",
        "text": "{\"tiddlers\":{\"$:/plugins/linonetwo/tidgi-language-model/Config/en-GB\":{\"title\":\"$:/plugins/linonetwo/tidgi-language-model/Config/en-GB\",\"type\":\"text/vnd.tiddlywiki\",\"text\":\"; Default model runner\\n: <$select tiddler='$:/plugins/linonetwo/tidgi-language-model/Config/default-runner'>\\n  <option value='llama.cpp'>llama.cpp</option>\\n  <option value='llm-rs'>llm-rs</option>\\n  <option value='rwkv.cpp'>rwkv.cpp</option>\\n</$select>\\n: For each runner's supported model, see [[https://llama-node.vercel.app/docs/model]]\\n;The number of CPU cores used by default\\n: <$edit-text tiddler='$:/plugins/linonetwo/tidgi-language-model/Config/cpu-count' tag='input' type='number' />\\n: Please use Task Manager to see how many cores your computer has.\\n\\n> Please note that some computers use Hyper-Threading or Big-Small cores, and one physical core may be treated as two logical cores by the operating system, etc. In this case, you should fill in the number of physical cores, or the larger the number, the slower the CPU will be when it exceeds a certain value.\\n\\n; Timeout (minutes)\\n: <$edit-text tiddler='$:/plugins/linonetwo/tidgi-language-model/Config/timeout' tag='input' type='number' />\\n: After waiting this long, if the language model doesn't say anything, abort the generation to prevent it from getting stuck in a dead loop. If the language model often aborts for no reason, you can increase this value.\"},\"$:/plugins/linonetwo/tidgi-language-model/Config/zh-Hans\":{\"title\":\"$:/plugins/linonetwo/tidgi-language-model/Config/zh-Hans\",\"type\":\"text/vnd.tiddlywiki\",\"text\":\"; 默认使用的语言模型运行器\\n: <$select tiddler='$:/plugins/linonetwo/tidgi-language-model/Config/default-runner'>\\n  <option value='llama.cpp'>llama.cpp</option>\\n  <option value='llm-rs'>llm-rs</option>\\n  <option value='rwkv.cpp'>rwkv.cpp</option>\\n</$select>\\n: 各运行器支持的模型见 [[https://llama-node.vercel.app/zh-Hans/docs/model]]\\n; 默认使用的CPU核心数\\n: <$edit-text tiddler='$:/plugins/linonetwo/tidgi-language-model/Config/cpu-count' tag='input' type='number' />\\n: 请使用任务管理器查看你的电脑拥有的核心数。\\n\\n> 要注意有的电脑使用了 Hyper-Threading 或大小核等技术，有一个物理核心可能会被操作系统视为两个逻辑核心等等情况，此时核心数应该填写物理核心数，不然超过某个值后，填得越大反而越慢。\\n\\n; 超时时间（分钟）\\n: <$edit-text tiddler='$:/plugins/linonetwo/tidgi-language-model/Config/timeout' tag='input' type='number' />\\n: 等待这么长时间之后，如果语言模型还一言不发，就中止生成，防止陷入死循环。如果语言模型经常无缘无故中止生成，可以适当增加这个值。\"},\"$:/plugins/linonetwo/tidgi-language-model/Config\":{\"title\":\"$:/plugins/linonetwo/tidgi-language-model/Config\",\"tags\":\"$:/tags/ControlPanel/SettingsTab\",\"type\":\"text/vnd.tiddlywiki\",\"caption\":\"TG AI\",\"text\":\"<$list filter=\\\"[[$:/language]get[text]removeprefix[$:/languages/]else[en-GB]]\\\" variable=\\\"lang\\\">\\n<$list filter=\\\"[<lang>search[zh]]\\\">\\n\\n{{$:/plugins/linonetwo/tidgi-language-model/Config/zh-Hans}}\\n\\n</$list>\\n\\n<$list filter=\\\"[<lang>!search[zh]]\\\">\\n\\n{{$:/plugins/linonetwo/tidgi-language-model/Config/en-GB}}\\n\\n</$list>\\n</$list>\\n\"},\"$:/plugins/linonetwo/tidgi-language-model/Config/cpu-count\":{\"title\":\"$:/plugins/linonetwo/tidgi-language-model/Config/cpu-count\",\"text\":\"4\"},\"$:/plugins/linonetwo/tidgi-language-model/Config/default-runner\":{\"title\":\"$:/plugins/linonetwo/tidgi-language-model/Config/default-runner\",\"text\":\"llama.cpp\"},\"$:/plugins/linonetwo/tidgi-language-model/Config/timeout\":{\"title\":\"$:/plugins/linonetwo/tidgi-language-model/Config/timeout\",\"text\":\"5\"},\"$:/plugins/linonetwo/tidgi-language-model/readme/en-GB\":{\"title\":\"$:/plugins/linonetwo/tidgi-language-model/readme/en-GB\",\"type\":\"text/vnd.tiddlywiki\",\"text\":\"Using LLaMa in TiddlyWiki.\\n\\nYou will have an additional \\\"TG AI\\\" page in your sidebar, where you can have a conversation directly, and the history of the conversation will be saved. To clear the history, simply delete the entry pointed to by the `history` parameter.\\n\\nLLaMa is actually a widget that allows you to customize the chatbot according to your needs:\\n\\n```html\\n<$tidgi-chat />\\n```\\n\\nVarious optional parameters can also be added to customize the behavior.\\n\\n|!Attributes |!Explanation |\\n|history |Fill in an tiddler title for persistent storage of chat logs |\\n|scroll |If yes, the conversation record can be scrolled up and down, but the height must be specified in the outer layer of the widget, refer to the [[sidebar|$:/plugins/linonetwo/tidgi-language-model/side-bar]] writing |\\n|component |DOM tag type for microware, default is div |\\n|className |Class name of the widget for custom styles |\\n|readonly |If it is readonly, no dialog input box will appear, and it will be used for display only with the history parameter. |\\n|system_message |System messages to customize the AI's behavior, such as \\\"You are an experienced lawyer\\\" |\\n\\nIn addition, the following LLaMa parameters are supported:\\n\\n```ts\\ninterface Generate {\\n  nThreads: number\\n  nTokPredict: number\\n  /**\\n   * logit bias for specific tokens\\n   * Default: None\\n   */\\n  logitBias?: Array<LogitBias>\\n  /**\\n   * top k tokens to sample from\\n   * Range: <= 0 to use vocab size\\n   * Default: 40\\n   */\\n  topK?: number\\n  /**\\n   * top p tokens to sample from\\n   * Default: 0.95\\n   * 1.0 = disabled\\n   */\\n  topP?: number\\n  /**\\n   * tail free sampling\\n   * Default: 1.0\\n   * 1.0 = disabled\\n   */\\n  tfsZ?: number\\n  /**\\n   * temperature\\n   * Default: 0.80\\n   * 1.0 = disabled\\n   */\\n  temp?: number\\n  /**\\n   * locally typical sampling\\n   * Default: 1.0\\n   * 1.0 = disabled\\n   */\\n  typicalP?: number\\n  /**\\n   * repeat penalty\\n   * Default: 1.10\\n   * 1.0 = disabled\\n   */\\n  repeatPenalty?: number\\n  /**\\n   * last n tokens to penalize\\n   * Default: 64\\n   * 0 = disable penalty, -1 = context size\\n   */\\n  repeatLastN?: number\\n  /**\\n   * frequency penalty\\n   * Default: 0.00\\n   * 1.0 = disabled\\n   */\\n  frequencyPenalty?: number\\n  /**\\n   * presence penalty\\n   * Default: 0.00\\n   * 1.0 = disabled\\n   */\\n  presencePenalty?: number\\n  /**\\n   * Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.\\n   * Mirostat: A Neural Text Decoding Algorithm that Directly Controls Perplexity\\n   * Default: 0\\n   * 0 = disabled\\n   * 1 = mirostat 1.0\\n   * 2 = mirostat 2.0\\n   */\\n  mirostat?: number\\n  /**\\n   * The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.\\n   * Default: 5.0\\n   */\\n  mirostatTau?: number\\n  /**\\n   * The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.\\n   * Default: 0.1\\n   */\\n  mirostatEta?: number\\n  /**\\n   * stop sequence\\n   * Default: None\\n   */\\n  stopSequence?: string\\n  /**\\n   * consider newlines as a repeatable token\\n   * Default: true\\n   */\\n  penalizeNl?: boolean\\n  /** prompt */\\n  prompt: string\\n}\\n```\\n\\n\\nor RWKV parameters:\\n\\n```ts\\ninterface RwkvInvocation {\\n  maxPredictLength: number\\n  topP: number\\n  temp: number\\n  endToken?: number\\n  endString?: string\\n  seed?: number\\n  prompt: string\\n  isSkipGeneration?: boolean\\n  sessionFilePath?: string\\n  isOverwriteSessionFile?: boolean\\n  presencePenalty?: number\\n  frequencyPenalty?: number\\n}\\n```\\n\\nIts specific usage can check the [[official documentation|https://llama-node.vercel.app/docs/backends/llama.cpp/inference]].\\n\\nNow there is no multi-round dialogue, even in a micro-piece chat, but also a single round of dialogue, multi-round dialogue and so on the next version to engage.\\n\\n!! Advance\\n\\nIf you nest your own action in the widget, you can get the result of the answer when the conversation is completed, which requires that you know how to write a widget that supports actions. The output is stored in the `output-text` variable.\\n\\nAt the same time, you can also catch bubbling events of the widget when the conversation completes, as well as global events, both using `addEventListener` and `$tw.hooks.addHook` (the event name is `tidgi-chat`) respectively. The following is the type definition of the event load.\\n\"},\"$:/plugins/linonetwo/tidgi-language-model/readme/zh-Hans\":{\"title\":\"$:/plugins/linonetwo/tidgi-language-model/readme/zh-Hans\",\"type\":\"text/vnd.tiddlywiki\",\"text\":\"在 TiddlyWiki 中使用 LLaMa。\\n\\n你的侧边栏会多出一个 TG AI 页面，可以直接进行对话，对话的历史会保存。如要清除历史记录，则直接删除 `history` 参数指向的条目。\\n\\nLLaMa 实际上是一个微件，你可以按照自己的需求定制聊天机器人：\\n\\n```html\\n<$tidgi-chat />\\n```\\n\\n还可以添加各种可选参数来定制行为：\\n\\n|!参数 |!解释 |\\n|history |填写一个条目的标题，用于持久化存储聊天记录 |\\n|component |微件的DOM标签类型，默认为div |\\n|className |微件的类名，用于自定义样式 |\\n|readonly |如果为readonly，则不会出现对话输入框，配合history参数仅做展示用 |\\n|system_message |系统消息，用于AI的行为，例如\\\"你是一个经验丰富的律师\\\" |\\n\\n除此之外，还支持如下 LLaMa 参数：\\n\\n```ts\\ninterface Generate {\\n  nThreads: number\\n  nTokPredict: number\\n  /**\\n   * logit bias for specific tokens\\n   * Default: None\\n   */\\n  logitBias?: Array<LogitBias>\\n  /**\\n   * top k tokens to sample from\\n   * Range: <= 0 to use vocab size\\n   * Default: 40\\n   */\\n  topK?: number\\n  /**\\n   * top p tokens to sample from\\n   * Default: 0.95\\n   * 1.0 = disabled\\n   */\\n  topP?: number\\n  /**\\n   * tail free sampling\\n   * Default: 1.0\\n   * 1.0 = disabled\\n   */\\n  tfsZ?: number\\n  /**\\n   * temperature\\n   * Default: 0.80\\n   * 1.0 = disabled\\n   */\\n  temp?: number\\n  /**\\n   * locally typical sampling\\n   * Default: 1.0\\n   * 1.0 = disabled\\n   */\\n  typicalP?: number\\n  /**\\n   * repeat penalty\\n   * Default: 1.10\\n   * 1.0 = disabled\\n   */\\n  repeatPenalty?: number\\n  /**\\n   * last n tokens to penalize\\n   * Default: 64\\n   * 0 = disable penalty, -1 = context size\\n   */\\n  repeatLastN?: number\\n  /**\\n   * frequency penalty\\n   * Default: 0.00\\n   * 1.0 = disabled\\n   */\\n  frequencyPenalty?: number\\n  /**\\n   * presence penalty\\n   * Default: 0.00\\n   * 1.0 = disabled\\n   */\\n  presencePenalty?: number\\n  /**\\n   * Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.\\n   * Mirostat: A Neural Text Decoding Algorithm that Directly Controls Perplexity\\n   * Default: 0\\n   * 0 = disabled\\n   * 1 = mirostat 1.0\\n   * 2 = mirostat 2.0\\n   */\\n  mirostat?: number\\n  /**\\n   * The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.\\n   * Default: 5.0\\n   */\\n  mirostatTau?: number\\n  /**\\n   * The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.\\n   * Default: 0.1\\n   */\\n  mirostatEta?: number\\n  /**\\n   * stop sequence\\n   * Default: None\\n   */\\n  stopSequence?: string\\n  /**\\n   * consider newlines as a repeatable token\\n   * Default: true\\n   */\\n  penalizeNl?: boolean\\n  /** prompt */\\n  prompt: string\\n}\\n```\\n\\n\\n或 RWKV 参数\\n\\n```ts\\ninterface RwkvInvocation {\\n  maxPredictLength: number\\n  topP: number\\n  temp: number\\n  endToken?: number\\n  endString?: string\\n  seed?: number\\n  prompt: string\\n  isSkipGeneration?: boolean\\n  sessionFilePath?: string\\n  isOverwriteSessionFile?: boolean\\n  presencePenalty?: number\\n  frequencyPenalty?: number\\n}\\n```\\n\\n其具体用法可以查看[[官方文档|https://llama-node.vercel.app/docs/backends/llama.cpp/inference]]。\\n\\n现在还没有做多轮对话，即便是在一个微件里聊的，也都是单轮对话，多轮对话等下个版本再搞。\\n\\n!! 高级\\n\\n如果在微件中嵌套自己的 action，就可以在对话完成时拿到回答的结果，这需要你知道该如何编写一个支持 action 的微件。输出的结果保存在 `output-text` 变量中。\\n\\n同时，在对话完成时你也可以捕获到微件的冒泡事件，以及全局事件，二者分别使用`addEventListener`和`$tw.hooks.addHook`(事件名称就是`tidgi-chat`)来实现。如下是事件负载的类型定义：\"},\"$:/plugins/linonetwo/tidgi-language-model/readme\":{\"title\":\"$:/plugins/linonetwo/tidgi-language-model/readme\",\"type\":\"text/vnd.tiddlywiki\",\"caption\":\"ChatGPT\",\"text\":\"<$list filter=\\\"[[$:/language]get[text]removeprefix[$:/languages/]else[en-GB]]\\\" variable=\\\"lang\\\">\\n<$list filter=\\\"[<lang>search[zh]]\\\">\\n\\n  {{$:/plugins/linonetwo/tidgi-language-model/readme/zh-Hans}}\\n\\n</$list>\\n\\n<$list filter=\\\"[<lang>!search[zh]]\\\">\\n  {{$:/plugins/linonetwo/tidgi-language-model/readme/en-GB}}\\n\\n</$list>\\n</$list>\\n\\n```typescript\\ninterface ChatCompletionEvent {\\n  event: Event;\\n  type: 'tidgi-chat';\\n  name: 'completion-finish';\\n  paramObject: {\\n    id: string;        // Chat id from OpenAI\\n    created: Date;     // Chat time\\n    assistant: string; // Answer of ChatGPT\\n    user: string;      // Your question\\n  };\\n  widget: ChatGPTWidget;\\n  historyTiddler: string;\\n}\\n```\"},\"$:/plugins/linonetwo/tidgi-language-model/side-bar\":{\"title\":\"$:/plugins/linonetwo/tidgi-language-model/side-bar\",\"type\":\"text/vnd.tiddlywiki\",\"tags\":\"$:/tags/SideBar\",\"caption\":\"TG AI\",\"text\":\"<$tidgi-chat history=\\\"$:/state/plugins/linonetwo/tidgi-language-model/side-bar-history\\\" scroll=\\\"yes\\\" temperature=\\\"1\\\" max_tokens=\\\"512\\\" system_message=\\\"You known much on TiddlyWiki. You should answer in wikitext format(`!` for title, and `#` for numeral list, `*` for unordered list.).\\\" />\\n\"},\"$:/plugins/linonetwo/tidgi-language-model/tidgi-chat-widget.js\":{\"title\":\"$:/plugins/linonetwo/tidgi-language-model/tidgi-chat-widget.js\",\"module-type\":\"widget\",\"type\":\"application/javascript\",\"Modern.TiddlyDev#Origin\":\"tidgi-chat-widget.ts\",\"text\":\"\\\"use strict\\\";var import_widget=require(\\\"$:/core/modules/widgets/widget.js\\\"),isChinese=()=>$tw.wiki.getTiddler(\\\"$:/language\\\").fields.text.includes(\\\"zh\\\"),renderConversation=({id:t,assistant:e,user:i,created:n,attachment:s},r,o,a,d,l,c)=>{let u;void 0!==l&&(u=$tw.utils.domMaker(\\\"button\\\",{\\\"class\\\":\\\"edit-button\\\",innerHTML:o,attributes:{title:r?\\\"重新生成问题\\\":\\\"Regenerate question\\\"}})).addEventListener(\\\"click\\\",()=>{l(i,s)});let h;void 0!==c&&(h=$tw.utils.domMaker(\\\"button\\\",{\\\"class\\\":\\\"delete-button\\\",innerHTML:a,attributes:{title:r?\\\"删除问题\\\":\\\"Delete question\\\"}})).addEventListener(\\\"click\\\",()=>{c()});var g=$tw.utils.domMaker(\\\"button\\\",{\\\"class\\\":\\\"copy-button\\\",innerHTML:d,attributes:{title:r?\\\"复制原文\\\":\\\"Copy raw text\\\"}});return g.addEventListener(\\\"click\\\",()=>{$tw.utils.copyToClipboard(e)}),$tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"chatgpt-conversation\\\",attributes:{\\\"chatgpt-conversation\\\":t},children:[$tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"chatgpt-conversation-message chatgpt-conversation-assistant\\\",children:[$tw.utils.domMaker(\\\"p\\\",{innerHTML:$tw.wiki.renderText(\\\"text/html\\\",\\\"text/vnd.tiddlywiki\\\",e)}),g]}),$tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"chatgpt-conversation-message chatgpt-conversation-user\\\",children:[$tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"conversation-datetime\\\",text:new Date(n).toLocaleString()}),$tw.utils.domMaker(\\\"p\\\",{text:i}),...s?[$tw.utils.domMaker(\\\"pre\\\",{text:s})]:[],...void 0===h?[]:[h],...void 0===u?[]:[u]]})]})},renderChatingConversation=(t,e,i,n,s,r)=>{var o=$tw.utils.domMaker(\\\"pre\\\",{text:t?\\\"思考中...\\\":\\\"Thinking...\\\",style:{background:\\\"transparent\\\",marginTop:\\\"0\\\",marginBottom:\\\"0\\\",padding:\\\"0\\\",border:\\\"none\\\"}});let a,d;void 0!==s&&(d=$tw.utils.domMaker(\\\"button\\\",{\\\"class\\\":\\\"cancel-button\\\",innerHTML:i,attributes:{title:t?\\\"中止生成\\\":\\\"Cancel generation\\\"}})).addEventListener(\\\"click\\\",()=>{s(a)});return{conversation:a=$tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"chatgpt-conversation chatgpt-conversation-chating\\\",children:[$tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"chatgpt-conversation-message chatgpt-conversation-assistant\\\",children:[$tw.utils.domMaker(\\\"p\\\",{children:[o]}),...void 0===d?[]:[d]]}),$tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"chatgpt-conversation-message chatgpt-conversation-user\\\",children:[$tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"conversation-datetime\\\",text:(new Date).toLocaleString()}),$tw.utils.domMaker(\\\"p\\\",{text:e}),$tw.utils.domMaker(\\\"pre\\\",{text:r})]})]}),answerBox:o,printError:t=>{a.remove(),n.append($tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"chatgpt-conversation chatgpt-conversation-error\\\",children:[$tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"chatgpt-conversation-message chatgpt-conversation-user\\\",children:[$tw.utils.domMaker(\\\"p\\\",{text:e})]}),$tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"chatgpt-conversation-message chatgpt-conversation-assistant\\\",text:t})]}))}}},historyManager=i=>({getHistory:()=>{let t=[];try{t=JSON.parse($tw.wiki.getTiddlerText(i)||\\\"[]\\\")}catch{}return t},setHistory:t=>{var e;$tw.wiki.addTiddler(new $tw.Tiddler(null!=(e=$tw.wiki.getTiddler(i))?e:{},{title:i,text:JSON.stringify(t),type:\\\"application/json\\\"}))}}),ChatGPTWidget=class extends import_widget.widget{constructor(){super(...arguments),this.containerNodeTag=\\\"div\\\",this.containerNodeClass=\\\"\\\",this.tmpHistoryTiddler=\\\"$:/temp/linonetwo/tidgi-language-model/history-\\\"+Date.now(),this.historyTiddler=this.tmpHistoryTiddler,this.chatButtonText=$tw.wiki.renderText(\\\"text/html\\\",\\\"text/vnd.tiddlywiki\\\",$tw.wiki.getTiddlerText(\\\"$:/core/images/add-comment\\\")),this.attachmentButtonText=$tw.wiki.renderText(\\\"text/html\\\",\\\"text/vnd.tiddlywiki\\\",$tw.wiki.getTiddlerText(\\\"$:/core/images/import-button\\\")),this.editButtonText=$tw.wiki.renderText(\\\"text/html\\\",\\\"text/vnd.tiddlywiki\\\",$tw.wiki.getTiddlerText(\\\"$:/core/images/edit-button\\\")),this.deleteButtonText=$tw.wiki.renderText(\\\"text/html\\\",\\\"text/vnd.tiddlywiki\\\",$tw.wiki.getTiddlerText(\\\"$:/core/images/delete-button\\\")),this.cancelButtonText=$tw.wiki.renderText(\\\"text/html\\\",\\\"text/vnd.tiddlywiki\\\",$tw.wiki.getTiddlerText(\\\"$:/core/images/cancel-button\\\")),this.copyButtonText=$tw.wiki.renderText(\\\"text/html\\\",\\\"text/vnd.tiddlywiki\\\",$tw.wiki.getTiddlerText(\\\"$:/core/images/copy-clipboard\\\")),this.scroll=!1,this.readonly=!1,this.runLanguageModelOptions={},this.runner=\\\"llama.cpp\\\",this.systemMessage=\\\"\\\"}initialise(t,e){super.initialise(t,e),this.computeAttributes()}execute(){this.containerNodeTag=this.getAttribute(\\\"component\\\",\\\"div\\\"),this.containerNodeClass=this.getAttribute(\\\"className\\\",\\\"\\\"),this.historyTiddler=this.getAttribute(\\\"history\\\",\\\"\\\")||this.tmpHistoryTiddler,this.scroll=\\\"yes\\\"===(null==(e=null==(t=this.getAttribute(\\\"scroll\\\"))?void 0:t.toLowerCase)?void 0:e.call(t)),this.readonly=\\\"yes\\\"===(null==(e=null==(t=this.getAttribute(\\\"readonly\\\"))?void 0:t.toLowerCase)?void 0:e.call(t)),this.runner=this.getAttribute(\\\"runner\\\",\\\"llama.cpp\\\");var t=Number(this.getAttribute(\\\"temp\\\")),e=Number(this.getAttribute(\\\"topP\\\")),i=Number.parseInt(this.getAttribute(\\\"maxPredictLength\\\"),10),n=Number(this.getAttribute(\\\"presencePenalty\\\")),s=Number(this.getAttribute(\\\"frequencyPenalty\\\"));this.runLanguageModelOptions={completionOptions:{}},Number.isSafeInteger(i)&&0<i&&(this.runLanguageModelOptions.completionOptions.maxPredictLength=i),0<=t&&t<=2&&(this.runLanguageModelOptions.completionOptions.temp=t),0<=e&&e<=1&&(this.runLanguageModelOptions.completionOptions.topP=e),-2<=n&&n<=2&&(this.runLanguageModelOptions.completionOptions.presencePenalty=n),-2<=s&&s<=2&&(this.runLanguageModelOptions.completionOptions.frequencyPenalty=s),this.systemMessage=this.getAttribute(\\\"system_message\\\",\\\"A chat between a user and an assistant. You are a helpful assistant.\\\\nUSER:\\\"),this.makeChildWidgets()}render(t,e){var i,n;void 0!==$tw.browser&&(this.execute(),i=$tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"conversations\\\"}),n=$tw.utils.domMaker(this.containerNodeTag,{\\\"class\\\":\\\"tidgi-language-model-container \\\"+this.containerNodeClass,children:[i]}),e?e.before(n):t.append(n),this.domNodes.push(n),this.chat(n,i))}refresh(t){var e=this.computeAttributes();return 0<$tw.utils.count(e)||null!=(e=t[this.historyTiddler])&&e.deleted?(this.refreshSelf(),!0):this.refreshChildren(t)}chat(t,k){try{const $=isChinese(),{getHistory:x,setHistory:b}=historyManager(this.historyTiddler);let T;if(!this.readonly){const y=$tw.utils.domMaker(\\\"textarea\\\",{\\\"class\\\":\\\"chat-input\\\",attributes:{type:\\\"text\\\",placeholder:$?\\\"输入一个问题...\\\":\\\"Ask a question...\\\",autofocus:!0,rows:1}}),M=(T=(t,e)=>{y.value=t,L.value=null!=e?e:\\\"\\\"},$tw.utils.domMaker(\\\"button\\\",{\\\"class\\\":\\\"chat-button\\\",innerHTML:this.chatButtonText,attributes:{title:$?\\\"进行对话\\\":\\\"Chat\\\"}}));var e=$tw.utils.domMaker(\\\"button\\\",{\\\"class\\\":\\\"attachment-button\\\",innerHTML:this.attachmentButtonText,attributes:{title:$?\\\"附加条目\\\":\\\"Attach Tiddler\\\"}});const L=$tw.utils.domMaker(\\\"input\\\",{\\\"class\\\":\\\"attachment-input\\\",attributes:{type:\\\"text\\\",placeholder:$?\\\"填入条目标题或筛选器表达式\\\":\\\"Fill in Tiddler title or filter expression\\\",autofocus:!1,hidden:!0}});t.prepend($tw.utils.domMaker(\\\"div\\\",{\\\"class\\\":\\\"chat-box\\\",children:[e,y,M]}));t.prepend(L);let v=!1;const n=a=>{var t,e,i;if(!v){const l=y.value.trim();if(l){var n=L.hidden?\\\"\\\":$tw.wiki.filterTiddlers(L.value).map(t=>$tw.wiki.getTiddlerText(t)).join(\\\"\\\\n\\\\n\\\");y.value=\\\"\\\",v=!0,M.disabled=!0;const c=$tw.wiki.getTiddlerText(\\\"$:/plugins/linonetwo/tidgi-language-model/Config/default-runner\\\",this.runner||\\\"llama.cpp\\\");var s=Number($tw.wiki.getTiddlerText(\\\"$:/plugins/linonetwo/tidgi-language-model/Config/cpu-count\\\",\\\"4\\\"))||4;const u=String(Date.now());let r=\\\"\\\",o=0;const h=t=>{var e;const i={id:u,created:o,assistant:r,user:l,attachment:L.hidden?\\\"\\\":L.value},n=(b([...x(),i]),t.remove(),renderConversation(i,$,this.editButtonText,this.deleteButtonText,this.copyButtonText,T,()=>{n.remove(),b(x().filter(({id:t})=>t!==i.id))}));k.prepend(n),this.setVariable(\\\"output-text\\\",r);var s={event:a,type:\\\"tidgi-chat\\\",name:\\\"completion-finish\\\",paramObject:{...i,created:new Date(1e3*i.created)},widget:this,historyTiddler:this.historyTiddler};null!=(e=this.invokeAction)&&e.call(this,this,s),this.dispatchEvent(s),$tw.hooks.invokeHook(\\\"tidgi-chat\\\",s),v=!1,M.disabled=!1};const{conversation:g,answerBox:p,printError:m}=renderChatingConversation($,l,this.cancelButtonText,k,async t=>{await window.service.languageModel.abortLanguageModel(c,u),v=!1,M.disabled=!1,t.remove()},n);if(k.prepend(g),void 0!==(null==(t=null==(t=null==window?void 0:window.observables)?void 0:t.languageModel)?void 0:t.runLanguageModel$))try{const w=t=>{console.error(t),m(String(t)),v=!1,M.disabled=!1};let t;switch(c){case\\\"llama.cpp\\\":t=window.observables.languageModel.runLanguageModel$(c,{completionOptions:{prompt:`CONTEXT:${n}\\n${this.systemMessage}\\nUSER:${l}\\nASSISTANT:`,...null==(e=this.runLanguageModelOptions)?void 0:e.completionOptions,nThreads:s},id:u});break;case\\\"llm-rs\\\":return void console.error(\\\"llm-rs runner Not implemented yet\\\");case\\\"rwkv.cpp\\\":t=window.observables.languageModel.runLanguageModel$(c,{completionOptions:{prompt:`CONTEXT:${n}\\n${this.systemMessage}\\nUSER:${l}\\nASSISTANT:`,...null==(i=this.runLanguageModelOptions)?void 0:i.completionOptions},loadConfig:{nThreads:s},id:u})}t.subscribe({next:t=>{var e;try{if(t.id!==u)return;r=\\\"\\\"+r+(null!=(e=t.token)?e:\\\"\\\"),p.textContent=r+\\\"█\\\",o=Date.now()}catch(i){w(i)}k.scrollTop=k.scrollHeight},error:w,complete:()=>{h(g)}})}catch(d){console.error(d),m(String(d))}}}};M.addEventListener(\\\"click\\\",n),e.addEventListener(\\\"click\\\",()=>{L.hidden=!L.hidden}),y.addEventListener(\\\"keydown\\\",t=>{t.isComposing||\\\"Enter\\\"!==t.code||t.shiftKey||(t.preventDefault(),n(t))})}for(const s of x()){const r=renderConversation(s,$,this.editButtonText,this.deleteButtonText,this.copyButtonText,T,this.readonly?void 0:()=>{r.remove(),b(x().filter(({id:t})=>t!==s.id))});k.append(r)}}catch(i){console.error(i),t.textContent=String(i)}}};exports[\\\"tidgi-chat\\\"]=ChatGPTWidget;\"},\"$:/plugins/linonetwo/tidgi-language-model/tidgi-chat-widget.css\":{\"title\":\"$:/plugins/linonetwo/tidgi-language-model/tidgi-chat-widget.css\",\"tags\":[\"$:/tags/Stylesheet\"],\"type\":\"text/css\",\"Modern.TiddlyDev#Origin\":\"../../temp_stylePlugin:ni:sha-256;umb6wjtVaMGBHlh-sC3_rl8epyY3-iuOMvYlBpYAJ2M\",\"text\":\".tidgi-language-model-container{height:100%;width:100%;display:flex;padding:10px 0;flex-direction:column}.tidgi-language-model-container .conversations{width:100%;flex-grow:1;overflow-y:auto}.tidgi-language-model-container .chat-box{width:100%;display:flex;border:1.5px solid #888a;border-radius:5px;background:#8881}.tidgi-language-model-container .chat-input{flex-grow:1;font-size:16px;min-height:3.5em;padding:1em .5em;overflow:hidden;resize:vertical}.tidgi-language-model-container .chat-button{font-size:20px}.tidgi-language-model-container .chatgpt-conversation{display:flex;flex-direction:column}.tidgi-language-model-container .chatgpt-conversation-assistant{background-image:linear-gradient(0deg,#8883,#8883)}.tidgi-language-model-container .chatgpt-conversation-error .chatgpt-conversation-assistant{color:red}.tidgi-language-model-container .chatgpt-conversation-user{font-weight:750;padding-top:18px!important}.tidgi-language-model-container .chatgpt-conversation-message{padding:10px 20px;position:relative;min-height:3.5em}.tidgi-language-model-container .chatgpt-conversation-message .conversation-datetime{position:absolute;left:0;top:0;-webkit-user-select:none;-moz-user-select:none;user-select:none;opacity:.5;font-size:12px;font-weight:500px}.tidgi-language-model-container .chatgpt-conversation-message .edit-button{position:absolute;right:0;top:18px}.tidgi-language-model-container .chatgpt-conversation-message .cancel-button,.tidgi-language-model-container .chatgpt-conversation-message .copy-button{position:absolute;right:0;top:5px}.tidgi-language-model-container .chatgpt-conversation-message .delete-button{position:absolute;right:40px;top:18px}\"}}}",
        "title": "$:/plugins/linonetwo/tidgi-language-model",
        "type": "application/json",
        "version": "0.1.0",
        "Modern.TiddlyDev#SHA256-Hashed": "d9a144c52b176282924c6d63ffa3118b5199cc76bef6f9091bfe6a8a56d9a1a0"
    }
]